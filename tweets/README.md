# Tweet Analysis Scripts

This folder contains scripts for analyzing tweets using embeddings, cosine similarity, and k-means clustering.

## Prerequisites

1. **Generate tweet embeddings first** using the modified `embedding_2.py` script:
   ```bash
   cd /Users/paigelee/Desktop/web-scraping-analysis
   source .venv/bin/activate
   python3 embedding_2.py "scraped_data/Tweets from Accounts.xlsx" -o "data/tweets_embeddings.npy"
   ```

2. **Required files**:
   - `../data/tweets_embeddings.npy` - Tweet embeddings generated by embedding_2.py
   - `../scraped_data/Tweets from Accounts.xlsx` - Original tweet data
   - `.env` file with OpenAI API key

## Scripts

### 1. Tweet Cosine Similarity Analysis (`tweet_cosine_analysis.py`)

Finds clusters of similar tweets using cosine similarity and graph-based clustering.

**Usage:**
```bash
cd tweets
python3 tweet_cosine_analysis.py
```

**Outputs:**
- `tweet_near_duplicate_clusters.json` - Raw cluster data
- `tweet_df_clusters_cosine.csv` - Cluster statistics with LLM labels
- `tweet_df_clusters_cosine.json` - JSON version of cluster data

**Features:**
- Groups tweets with cosine similarity â‰¥ 0.75
- Preserves tweet_id throughout the process
- Generates political discourse labels using LLM
- Performs sentiment analysis on clusters
- Tracks author diversity within clusters

### 2. Tweet K-means Clustering (`tweet_kmeans_clustering.py`)

Performs k-means clustering on tweet embeddings with automatic optimal k selection.

**Usage:**
```bash
cd tweets
python3 tweet_kmeans_clustering.py
```

**Outputs:**
- `tweet_kmeans_clusters.json` - Raw cluster data
- `tweet_df_clusters_kmeans.csv` - Cluster statistics with LLM labels
- `tweet_df_clusters_kmeans.json` - JSON version of cluster data
- `tweet_kmeans_clustering_analysis.png` - Visualization of clustering results

**Features:**
- Automatically determines optimal number of clusters using elbow method and silhouette analysis
- Preserves tweet_id throughout the process
- Generates political discourse labels using LLM
- Performs sentiment analysis on clusters
- Creates comprehensive visualizations
- Tracks author diversity within clusters

### 3. Tweet Language Detection (`tweet_language_detection.py`)

Detects the language of tweets using OpenAI's GPT-4 model.

**Usage:**
```bash
cd tweets
python3 tweet_language_detection.py
```

**Prerequisites:**
- Set `OPENAI_API_KEY` environment variable
- Requires `../embeddings/clean_tweet_texts_by_id.json` file

**Outputs:**
- `data/tweet_languages.csv` - CSV with tweet_id and detected language

**Features:**
- Uses GPT-4o-mini for cost-effective and accurate language detection
- Handles edge cases (URLs only, emojis only, etc.) with "No language detected"
- Batch processing with rate limiting (20 tweets per API call)
- Comprehensive error handling
- Supports multiple languages including Swahili, French, English, Arabic, etc.

## Key Features

### Tweet ID Preservation
Both scripts maintain the original `tweet_id` from the Excel file throughout the entire analysis pipeline, ensuring you can always trace back to the original tweets.

### LLM-Powered Analysis
- **Political Discourse Labels**: Each cluster gets a descriptive label focusing on political themes, actors, and discourse patterns
- **Sentiment Analysis**: Clusters are scored from -1.0 (very negative) to 1.0 (very positive)
- **Batch Processing**: Efficient processing of multiple clusters in single API calls

### Author Diversity Tracking
- Tracks unique authors per cluster
- Shows usernames and author IDs
- Measures cluster diversity by number of unique authors

## Output Data Structure

### Cluster CSV/JSON Files
Each cluster record contains:
- `cluster_id`: Unique identifier for the cluster
- `num_tweets`: Number of tweets in the cluster
- `num_authors`: Number of unique authors in the cluster
- `authors`: List of author IDs
- `usernames`: List of usernames
- `tweet_ids`: List of all tweet IDs in the cluster
- `tweets`: List of tweet texts
- `llm_label`: AI-generated political discourse label
- `sentiment_score`: Sentiment score (-1.0 to 1.0)

## Running the Analysis

1. **First, generate embeddings:**
   ```bash
   cd /Users/paigelee/Desktop/web-scraping-analysis
   source .venv/bin/activate
   python3 embedding_2.py "scraped_data/Tweets from Accounts.xlsx" -o "data/tweets_embeddings.npy"
   ```

2. **Then run either analysis:**
   ```bash
   cd tweets
   python3 tweet_cosine_analysis.py    # For similarity-based clustering
   # OR
   python3 tweet_kmeans_clustering.py  # For k-means clustering
   ```

## Notes

- Both scripts require an OpenAI API key in your `.env` file
- The scripts will create the `tweets/` directory if it doesn't exist
- All output files are saved in the `tweets/` directory
- The analysis preserves the original tweet structure and IDs for easy reference back to the source data
